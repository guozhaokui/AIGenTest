#!/usr/bin/env python3
"""
ç«¯åˆ°ç«¯Demoï¼šä»æ–‡æ¡£è¯»å–åˆ°æ™ºèƒ½é—®ç­”

æ¼”ç¤ºæµç¨‹ï¼š
1. è¯»å–çœŸå®æ–‡æ¡£ï¼ˆ2601.mdï¼‰
2. å‘é‡åŒ–å¹¶å­˜å‚¨
3. ç”¨æˆ·æé—®
4. æ£€ç´¢ç›¸å…³æ–‡æ¡£
5. è°ƒç”¨LLMç”Ÿæˆå›ç­”
"""

import sys
import os
from pathlib import Path
from typing import List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from core.vector_store import VectorStore, Document
from core.embedding import create_embedding_provider


class SimpleRAGDemo:
    """ç®€å•çš„RAGæ¼”ç¤ºç³»ç»Ÿ"""

    def __init__(self, docs_path: str = "/mnt/e/TEST/work/æ—¥å¿—"):
        """
        Args:
            docs_path: æ–‡æ¡£ç›®å½•è·¯å¾„
        """
        self.docs_path = Path(docs_path)

        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        print("=" * 60)
        print("åˆå§‹åŒ–RAGç³»ç»Ÿ")
        print("=" * 60)

        self.vector_store = VectorStore(
            path=".memory_db/demo_vectors",
            collection_name="demo"
        )

        # æ¸…ç©ºä¹‹å‰çš„æ•°æ®ï¼ˆdemoï¼‰
        if self.vector_store.count() > 0:
            print(f"\næ¸…ç†ä¹‹å‰çš„demoæ•°æ®ï¼ˆ{self.vector_store.count()}æ¡ï¼‰...")
            self.vector_store.clear()

        # åˆå§‹åŒ–NVIDIA APIï¼ˆç”¨äºç”Ÿæˆå›ç­”ï¼‰
        self.setup_llm()

    def setup_llm(self):
        """åˆå§‹åŒ–LLM API"""
        from dotenv import load_dotenv
        import openai

        # åŠ è½½ç¯å¢ƒå˜é‡
        env_path = Path(__file__).parent.parent / '.env'
        load_dotenv(dotenv_path=env_path)

        api_key = os.getenv('NVIDIA_API_KEY')
        if not api_key:
            print("âš ï¸ æœªæ‰¾åˆ°NVIDIA_API_KEYï¼Œå°†æ— æ³•ç”Ÿæˆå›ç­”")
            self.llm_client = None
            return

        self.llm_client = openai.OpenAI(
            base_url="https://integrate.api.nvidia.com/v1",
            api_key=api_key
        )

        print("âœ“ NVIDIA APIå·²åˆå§‹åŒ–")

    def load_documents(self):
        """åŠ è½½æ–‡æ¡£ç›®å½•ä¸‹çš„æ‰€æœ‰markdownæ–‡ä»¶"""
        print("\n" + "=" * 60)
        print("åŠ è½½æ–‡æ¡£")
        print("=" * 60)

        if not self.docs_path.exists():
            print(f"âš ï¸ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {self.docs_path}")
            print("  ä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£...")
            return self.load_example_documents()

        md_files = list(self.docs_path.glob("*.md"))
        print(f"\næ‰¾åˆ° {len(md_files)} ä¸ªMarkdownæ–‡ä»¶")

        documents = []
        for md_file in md_files[:5]:  # é™åˆ¶åªåŠ è½½å‰5ä¸ªæ–‡ä»¶ï¼ˆdemoï¼‰
            try:
                content = md_file.read_text(encoding='utf-8')
                print(f"  âœ“ {md_file.name} ({len(content)} å­—ç¬¦)")

                documents.append(Document(
                    content=content,
                    metadata={
                        "source": md_file.name,
                        "path": str(md_file),
                        "type": "daily_log"
                    }
                ))
            except Exception as e:
                print(f"  âœ— {md_file.name}: {e}")

        return documents

    def load_example_documents(self):
        """åŠ è½½ç¤ºä¾‹æ–‡æ¡£ï¼ˆå¦‚æœæ‰¾ä¸åˆ°çœŸå®æ–‡æ¡£ï¼‰"""
        print("\nä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£...")

        example_content = """
0107
MetaGPT
    åœ¨wslç¯å¢ƒä¸‹
    ~/work$ conda create -n metagpt python=3.9
    ~/work$ conda activate metagpt
    ~/work/MetaGPT$ pip install -e .
    ~/work/MetaGPT$ metagpt --init-config
    Configuration file initialized at /home/guozhaokui/.metagpt/config2.yaml
    ~/work/MetaGPT$ python -m metagpt.webserver.run --reload
    ğŸŒ åœ°å€: http://0.0.0.0:8000

Claude Code
    cursorçš„å¯¹è¯è®°å½•å¯¹åº” wslçš„homeç›®å½•
    server
    åœ¨wslçš„ /home/guozhaokui/work/testcode/claudeserver
    éœ€è¦å…ˆéƒ¨ç½²åˆ°usaæœåŠ¡å™¨ï¼Œç„¶ååœ¨é‚£ä¸ªæœåŠ¡å™¨ä¸Šæ‰§è¡Œserver.py
    claude codeçš„é…ç½®åœ¨ ~/.claude$ nano settings.json
    ~/work/testcode/claudeserver$ python test_thinking_cache.py
    è¿™ä¸ªthinkingçš„æµ‹è¯•æ²¡æœ‰é€šè¿‡

linux81
~/laya/guo/AIGenTest/aiserver/test/QAMath$ python build_index.py ç”Ÿæˆç´¢å¼•
(qwen) layabox@layabox-System-Product-Name:~/laya/guo/AIGenTest/aiserver/test/QAMath$ python server.py
å› ä¸ºæœ‰Qwen8Bæ¨¡å‹
start_8b.sh
stop_8b.sh

sam3Dæµ‹è¯•
    8å¡3090
    conda activate sam3d
    /data1/guo/AIGenTest/aiserver/sam3d/start_web.sh

linux21
    æœ‰ä¸€ä¸ªiquestç¯å¢ƒï¼Œä¸‹è½½äº† mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-4bit æ¨¡å‹
    è¿™ä¸ªæ¨¡å‹æ˜¯ç»™macç”¨çš„ï¼Œæ‰€ä»¥å¤±è´¥äº†
    (hidream) ubuntu@ubuntu21:/mnt/hdd/guo/AIGenTest/aiserver/test$ python ./dinov3_server.py
å¯åŠ¨ DINOv3 å¯è§†åŒ–æœåŠ¡ï¼Œç«¯å£: 6020
è®¿é—® http://localhost:6020

(base) ubuntu@ubuntu21:/mnt/hdd/guo/AIGenTest/aiserver/embedding$ ./start_embed_server.sh
    BGE
    siglip2

windows claude code
set HTTPS_PROXY=http://127.0.0.1:10809
export https_proxy=http://127.0.0.1:10809

åœ¨gitbashä¸‹å®‰è£…
$ export CLAUDE_CODE_GIT_BASH_PATH="D:\\Program Files\\Git\\git-bash.exe"
        """

        return [Document(
            content=example_content,
            metadata={
                "source": "ç¤ºä¾‹-2601.md",
                "type": "daily_log"
            }
        )]

    def index_documents(self, documents: List[Document]):
        """ç´¢å¼•æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
        print("\n" + "=" * 60)
        print("ç´¢å¼•æ–‡æ¡£")
        print("=" * 60)

        print(f"\næ­£åœ¨ç´¢å¼• {len(documents)} ä¸ªæ–‡æ¡£...")

        for doc in documents:
            # è‡ªåŠ¨åˆ†å—å¹¶æ·»åŠ 
            doc_ids = self.vector_store.add_document(
                content=doc.content,
                metadata=doc.metadata,
                chunk=True  # å¯ç”¨åˆ†å—
            )

            source = doc.metadata.get('source', 'unknown')
            print(f"  âœ“ {source}: åˆ†æˆ {len(doc_ids)} ä¸ªå—")

        total = self.vector_store.count()
        print(f"\nâœ“ ç´¢å¼•å®Œæˆï¼Œæ€»æ–‡æ¡£æ•°: {total}")

    def search(self, query: str, top_k: int = 3):
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        print(f"\nğŸ” æ£€ç´¢: {query}")
        print("-" * 60)

        results = self.vector_store.search(query, top_k=top_k)

        print(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£:\n")

        for i, result in enumerate(results, 1):
            print(f"{i}. [ç›¸ä¼¼åº¦: {result.similarity:.3f}] {result.metadata.get('source', 'unknown')}")
            # æ˜¾ç¤ºå†…å®¹ç‰‡æ®µ
            content_preview = result.content.replace('\n', ' ')[:80]
            print(f"   {content_preview}...")
            print()

        return results

    def generate_answer(self, query: str, context_docs):
        """ä½¿ç”¨LLMç”Ÿæˆå›ç­”"""
        if not self.llm_client:
            print("âš ï¸ LLMæœªåˆå§‹åŒ–ï¼Œæ— æ³•ç”Ÿæˆå›ç­”")
            return None

        print("ğŸ¤– ç”Ÿæˆå›ç­”...")
        print("-" * 60)

        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"ã€æ–‡æ¡£{i+1}ã€‘æ¥æº: {doc.metadata.get('source')}\n{doc.content}"
            for i, doc in enumerate(context_docs)
        ])

        # æ„å»ºprompt
        prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚

ã€æ–‡æ¡£å†…å®¹ã€‘
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€å›ç­”è¦æ±‚ã€‘
1. åªåŸºäºæ–‡æ¡£å†…å®¹å›ç­”ï¼Œä¸è¦æ·»åŠ æ–‡æ¡£å¤–çš„ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜
3. å›ç­”è¦ç®€æ´æ˜äº†
4. æ ‡æ³¨ä¿¡æ¯æ¥æºï¼ˆå“ªä¸ªæ–‡æ¡£ï¼‰

ã€å›ç­”ã€‘"""

        try:
            # è°ƒç”¨NVIDIA API
            completion = self.llm_client.chat.completions.create(
                model="deepseek-ai/deepseek-v3.2",
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1024,
                temperature=0.3
            )

            answer = completion.choices[0].message.content
            print(answer)
            print()

            return answer

        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
            return None

    def query(self, question: str):
        """å®Œæ•´çš„æŸ¥è¯¢æµç¨‹"""
        print("\n" + "=" * 60)
        print(f"é—®é¢˜: {question}")
        print("=" * 60)

        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = self.search(question, top_k=3)

        if not docs:
            print("âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            return

        # 2. ç”Ÿæˆå›ç­”
        answer = self.generate_answer(question, docs)

        return {
            "question": question,
            "documents": docs,
            "answer": answer
        }

    def interactive_mode(self):
        """äº¤äº’æ¨¡å¼"""
        print("\n" + "=" * 60)
        print("äº¤äº’å¼é—®ç­”ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰")
        print("=" * 60)

        while True:
            try:
                question = input("\nğŸ’¬ ä½ çš„é—®é¢˜: ").strip()

                if not question:
                    continue

                if question.lower() in ['quit', 'exit', 'q']:
                    print("\nå†è§ï¼")
                    break

                self.query(question)

            except KeyboardInterrupt:
                print("\n\nå†è§ï¼")
                break
            except Exception as e:
                print(f"\nâŒ é”™è¯¯: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸš€ " * 20)
    print("ç«¯åˆ°ç«¯RAG Demo")
    print("ğŸš€ " * 20)

    # åˆ›å»ºdemoç³»ç»Ÿ
    demo = SimpleRAGDemo(docs_path="/mnt/e/TEST/work/æ—¥å¿—")

    # 1. åŠ è½½æ–‡æ¡£
    documents = demo.load_documents()

    if not documents:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£")
        return

    # 2. ç´¢å¼•æ–‡æ¡£
    demo.index_documents(documents)

    # 3. æµ‹è¯•æŸ¥è¯¢
    print("\n" + "=" * 60)
    print("æµ‹è¯•æŸ¥è¯¢")
    print("=" * 60)

    test_queries = [
        "MetaGPTæ€ä¹ˆå¯åŠ¨ï¼Ÿ",
        "QAMathåœ¨å“ªä¸ªæœåŠ¡å™¨ä¸Šï¼Ÿ",
        "æœ‰å“ªäº›æœåŠ¡å™¨ï¼Ÿ",
        "Claude Codeçš„é…ç½®åœ¨å“ªé‡Œï¼Ÿ"
    ]

    for query in test_queries:
        demo.query(query)
        print("\n" + "-" * 60)
        input("æŒ‰å›è½¦ç»§ç»­ä¸‹ä¸€ä¸ªé—®é¢˜...")

    # 4. è¿›å…¥äº¤äº’æ¨¡å¼
    print("\n\n")
    choice = input("æ˜¯å¦è¿›å…¥äº¤äº’æ¨¡å¼ï¼Ÿ(y/n): ").strip().lower()

    if choice == 'y':
        demo.interactive_mode()

    print("\n" + "=" * 60)
    print("Demoç»“æŸ")
    print("=" * 60)


if __name__ == "__main__":
    main()
