#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆDemoï¼šæµ‹è¯•2601.mdçš„é—®ç­”

å¿«é€ŸéªŒè¯RAGæµç¨‹
"""

import sys
import os
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))

from core.vector_store import VectorStore, Document
from dotenv import load_dotenv
import openai


def main():
    print("\n" + "=" * 60)
    print("ç®€åŒ–ç‰ˆRAG Demo - æµ‹è¯•2601.md")
    print("=" * 60)

    # 1. å‡†å¤‡æµ‹è¯•æ–‡æ¡£ï¼ˆæ¨¡æ‹Ÿ2601.mdï¼‰
    test_content = """
0107
MetaGPT
    åœ¨wslç¯å¢ƒä¸‹
    ~/work$ conda create -n metagpt python=3.9
    ~/work$ conda activate metagpt
    ~/work/MetaGPT$ pip install -e .
    ~/work/MetaGPT$ metagpt --init-config
    Configuration file initialized at /home/guozhaokui/.metagpt/config2.yaml
    ~/work/MetaGPT$ python -m metagpt.webserver.run --reload
    ğŸŒ åœ°å€: http://0.0.0.0:8000

Claude Code
    cursorçš„å¯¹è¯è®°å½•å¯¹åº” wslçš„homeç›®å½•
    serveråœ¨wslçš„ /home/guozhaokui/work/testcode/claudeserver
    éœ€è¦å…ˆéƒ¨ç½²åˆ°usaæœåŠ¡å™¨ï¼Œç„¶ååœ¨é‚£ä¸ªæœåŠ¡å™¨ä¸Šæ‰§è¡Œserver.py
    claude codeçš„é…ç½®åœ¨ ~/.claude/settings.json

linux81
~/laya/guo/AIGenTest/aiserver/test/QAMath$ python build_index.py ç”Ÿæˆç´¢å¼•
(qwen) layabox@layabox-System-Product-Name:~/laya/guo/AIGenTest/aiserver/test/QAMath$ python server.py
å› ä¸ºæœ‰Qwen8Bæ¨¡å‹
start_8b.sh

sam3Dæµ‹è¯•
    8å¡3090
    conda activate sam3d
    /data1/guo/AIGenTest/aiserver/sam3d/start_web.sh

linux21
    (hidream) ubuntu@ubuntu21:/mnt/hdd/guo/AIGenTest/aiserver/test$ python ./dinov3_server.py
    å¯åŠ¨ DINOv3 å¯è§†åŒ–æœåŠ¡ï¼Œç«¯å£: 6020
    è®¿é—® http://localhost:6020

(base) ubuntu@ubuntu21:/mnt/hdd/guo/AIGenTest/aiserver/embedding$ ./start_embed_server.sh
    BGEåµŒå…¥æœåŠ¡ï¼Œç«¯å£: 6012
    SigLIP-2å›¾ç‰‡åµŒå…¥ï¼Œç«¯å£: 6010
    """

    # 2. åˆå§‹åŒ–å‘é‡å­˜å‚¨
    print("\n[æ­¥éª¤1] åˆå§‹åŒ–å‘é‡å­˜å‚¨...")
    store = VectorStore(path=".memory_db/demo_simple", collection_name="simple_demo")
    store.clear()

    # 3. ç´¢å¼•æ–‡æ¡£
    print("\n[æ­¥éª¤2] ç´¢å¼•æ–‡æ¡£...")
    doc_ids = store.add_document(
        content=test_content,
        metadata={"source": "2601.md", "date": "2024-01-07"},
        chunk=True
    )
    print(f"  âœ“ æ–‡æ¡£åˆ†æˆ {len(doc_ids)} ä¸ªå—")
    print(f"  âœ“ æ€»æ–‡æ¡£æ•°: {store.count()}")

    # 4. æµ‹è¯•æŸ¥è¯¢
    print("\n[æ­¥éª¤3] æµ‹è¯•æŸ¥è¯¢...")
    print("=" * 60)

    test_queries = [
        "MetaGPTæ€ä¹ˆå¯åŠ¨ï¼Ÿ",
        "QAMathåœ¨å“ªä¸ªæœåŠ¡å™¨ä¸Šï¼Ÿ",
        "BGEåµŒå…¥æœåŠ¡çš„ç«¯å£æ˜¯å¤šå°‘ï¼Ÿ",
        "linux21ä¸Šè¿è¡Œä»€ä¹ˆæœåŠ¡ï¼Ÿ"
    ]

    for query in test_queries:
        print(f"\nğŸ’¬ é—®é¢˜: {query}")
        print("-" * 60)

        # æ£€ç´¢
        results = store.search(query, top_k=2)

        print(f"æ£€ç´¢ç»“æœï¼ˆTop-{len(results)}ï¼‰:\n")
        for i, result in enumerate(results, 1):
            print(f"{i}. [ç›¸ä¼¼åº¦: {result.similarity:.3f}]")
            # æ˜¾ç¤ºç‰‡æ®µ
            lines = result.content.strip().split('\n')[:3]
            for line in lines:
                if line.strip():
                    print(f"   {line}")
            print()

    # 5. æµ‹è¯•LLMå›ç­”ï¼ˆå¦‚æœé…ç½®äº†APIï¼‰
    print("\n[æ­¥éª¤4] æµ‹è¯•LLMç”Ÿæˆå›ç­”...")
    print("=" * 60)

    # åŠ è½½ç¯å¢ƒå˜é‡
    env_path = Path(__file__).parent.parent / '.env'
    load_dotenv(dotenv_path=env_path)

    api_key = os.getenv('NVIDIA_API_KEY')
    if not api_key:
        print("âš ï¸ æœªæ‰¾åˆ°NVIDIA_API_KEYï¼Œè·³è¿‡LLMæµ‹è¯•")
        print("\nâœ… Demoå®Œæˆï¼ˆå‘é‡æ£€ç´¢éƒ¨åˆ†ï¼‰")
        return

    # åˆå§‹åŒ–LLM
    client = openai.OpenAI(
        base_url="https://integrate.api.nvidia.com/v1",
        api_key=api_key
    )

    # æµ‹è¯•ä¸€ä¸ªé—®é¢˜
    query = "QAMathåœ¨å“ªä¸ªæœåŠ¡å™¨ä¸Šï¼Ÿæ€ä¹ˆå¯åŠ¨ï¼Ÿ"
    print(f"\nğŸ’¬ é—®é¢˜: {query}")
    print("-" * 60)

    # æ£€ç´¢ä¸Šä¸‹æ–‡
    results = store.search(query, top_k=3)
    context = "\n\n".join([r.content for r in results])

    print("æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡:")
    print(context[:200] + "...\n")

    # æ„å»ºprompt
    prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜ã€‚

ã€æ–‡æ¡£å†…å®¹ã€‘
{context}

ã€é—®é¢˜ã€‘
{query}

ã€å›ç­”ã€‘ï¼ˆç®€æ´æ˜äº†ï¼Œæ ‡æ³¨æ¥æºï¼‰"""

    # ç”Ÿæˆå›ç­”
    print("ğŸ¤– LLMå›ç­”:")
    print("-" * 60)

    try:
        completion = client.chat.completions.create(
            model="deepseek-ai/deepseek-v3.2",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=512,
            temperature=0.3
        )

        answer = completion.choices[0].message.content
        print(answer)
        print()

    except Exception as e:
        print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")

    # 6. ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 60)
    print("ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 60)

    stats = store.stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")

    print("\nâœ… Demoå®Œæˆï¼")
    print("\næç¤º: è¿è¡Œ python demo_e2e.py ä½“éªŒå®Œæ•´ç‰ˆ")
    print("=" * 60)


if __name__ == "__main__":
    main()
