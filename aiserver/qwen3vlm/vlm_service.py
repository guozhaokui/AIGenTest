#!/usr/bin/env python3
"""
Qwen3-VL Vision Language Model æœåŠ¡
åŸºäº transformers + FastAPIï¼Œæä¾› OpenAI å…¼å®¹çš„ API

ä½¿ç”¨æ–¹æ³•:
    conda activate qwen
    python vlm_service.py [--port 8080] [--gpu 0]
"""

import argparse
import base64
import os
import re
import time
import torch
import uvicorn
from contextlib import asynccontextmanager
from io import BytesIO
from pathlib import Path
from typing import List, Optional, Union

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from PIL import Image
import requests as http_requests

# é»˜è®¤æ¨¡å‹è·¯å¾„
DEFAULT_MODEL_PATH = "/data1/MLLM/qwen2.5vl/Qwen/Qwen/Qwen3-VL-8B-Instruct"

# å…¨å±€æ¨¡å‹å’Œå¤„ç†å™¨
model = None
processor = None


class ImageUrl(BaseModel):
    url: str


class ContentItem(BaseModel):
    type: str
    text: Optional[str] = None
    image_url: Optional[ImageUrl] = None
    image: Optional[str] = None  # å…¼å®¹ Qwen æ ¼å¼


class Message(BaseModel):
    role: str
    content: Union[str, List[ContentItem]]


class ChatRequest(BaseModel):
    model: str = "qwen3-vl"
    messages: List[Message]
    max_tokens: Optional[int] = 1024
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.8
    top_k: Optional[int] = 20
    stream: Optional[bool] = False


class Choice(BaseModel):
    index: int
    message: Message
    finish_reason: str


class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class ChatResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Choice]
    usage: Usage


def load_model(model_path: str):
    """åŠ è½½æ¨¡å‹"""
    global model, processor
    
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
    
    from transformers import AutoModelForVision2Seq, AutoProcessor
    
    # ä½¿ç”¨ AutoModelForVision2Seq è‡ªåŠ¨åŠ è½½æ­£ç¡®çš„æ¨¡å‹ç±»
    # Qwen3-VL ä½¿ç”¨ Qwen3VLForConditionalGeneration
    # Qwen2.5-VL ä½¿ç”¨ Qwen2_5_VLForConditionalGeneration
    
    # å°è¯•ä½¿ç”¨ flash_attention_2
    try:
        model = AutoModelForVision2Seq.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            attn_implementation="flash_attention_2",
            device_map="auto",
            trust_remote_code=True,
        )
        print("âœ… ä½¿ç”¨ Flash Attention 2")
    except Exception as e:
        print(f"âš ï¸ Flash Attention ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æ³¨æ„åŠ›: {e}")
        model = AutoModelForVision2Seq.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
        )
    
    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
    
    # å°è¯•ä½¿ç”¨ torch.compile ä¼˜åŒ–ï¼ˆPyTorch 2.0+ï¼‰
    try:
        model = torch.compile(model, mode="reduce-overhead")
        print("âœ… å·²å¯ç”¨ torch.compile ä¼˜åŒ–")
    except Exception as e:
        print(f"âš ï¸ torch.compile ä¸å¯ç”¨: {e}")
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ! æ¨¡å‹ç±»å‹: {type(model).__name__}")
    return model, processor


def convert_messages_to_qwen_format(messages: List[Message]) -> list:
    """å°† OpenAI æ ¼å¼çš„æ¶ˆæ¯è½¬æ¢ä¸º Qwen æ ¼å¼"""
    qwen_messages = []
    
    for msg in messages:
        qwen_msg = {"role": msg.role, "content": []}
        
        if isinstance(msg.content, str):
            qwen_msg["content"] = [{"type": "text", "text": msg.content}]
        else:
            for item in msg.content:
                if item.type == "text":
                    qwen_msg["content"].append({
                        "type": "text",
                        "text": item.text
                    })
                elif item.type == "image_url" and item.image_url:
                    qwen_msg["content"].append({
                        "type": "image",
                        "image": item.image_url.url
                    })
                elif item.type == "image" and item.image:
                    qwen_msg["content"].append({
                        "type": "image",
                        "image": item.image
                    })
        
        qwen_messages.append(qwen_msg)
    
    return qwen_messages


def generate_response(messages: List[Message], **kwargs) -> tuple:
    """ç”Ÿæˆå›å¤"""
    from qwen_vl_utils import process_vision_info
    
    # è½¬æ¢æ¶ˆæ¯æ ¼å¼
    qwen_messages = convert_messages_to_qwen_format(messages)
    
    # å‡†å¤‡è¾“å…¥
    text = processor.apply_chat_template(
        qwen_messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    image_inputs, video_inputs = process_vision_info(qwen_messages)
    
    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    )
    inputs = inputs.to(model.device)
    
    # ç”Ÿæˆå‚æ•°
    gen_kwargs = {
        "max_new_tokens": kwargs.get("max_tokens", 1024),
        "temperature": kwargs.get("temperature", 0.7),
        "top_p": kwargs.get("top_p", 0.8),
        "top_k": kwargs.get("top_k", 20),
        "do_sample": kwargs.get("temperature", 0.7) > 0,
    }
    
    # ç”Ÿæˆ
    with torch.no_grad():
        generated_ids = model.generate(**inputs, **gen_kwargs)
    
    # è§£ç è¾“å‡º
    generated_ids_trimmed = [
        out_ids[len(in_ids):] 
        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    
    output_text = processor.batch_decode(
        generated_ids_trimmed, 
        skip_special_tokens=True, 
        clean_up_tokenization_spaces=False
    )[0]
    
    # è®¡ç®— token æ•°é‡
    prompt_tokens = inputs.input_ids.shape[1]
    completion_tokens = generated_ids_trimmed[0].shape[0]
    
    return output_text, prompt_tokens, completion_tokens


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    model_path = os.environ.get("MODEL_PATH", DEFAULT_MODEL_PATH)
    load_model(model_path)
    yield


app = FastAPI(title="Qwen3-VL API", lifespan=lifespan)

# CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ==================== Caption API (å…¼å®¹ imagemgr) ====================

class CaptionRequest(BaseModel):
    """å›¾ç‰‡æè¿°è¯·æ±‚ï¼ˆå…¼å®¹ imagemgr è°ƒç”¨æ ¼å¼ï¼‰"""
    image_base64: str
    prompt: Optional[str] = "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦ç‰©ä½“ã€åœºæ™¯ã€é¢œè‰²ã€é£æ ¼ç­‰ç‰¹å¾ã€‚"
    max_tokens: Optional[int] = 512
    temperature: Optional[float] = 0.7


class CaptionResponse(BaseModel):
    """å›¾ç‰‡æè¿°å“åº”"""
    caption: str
    prompt_tokens: Optional[int] = None
    completion_tokens: Optional[int] = None


@app.post("/caption")
async def generate_caption(request: CaptionRequest):
    """
    ç”Ÿæˆå›¾ç‰‡æè¿° APIï¼ˆå…¼å®¹ imagemgr è°ƒç”¨ï¼‰
    
    è¯·æ±‚æ ¼å¼:
        {"image_base64": "...", "prompt": "æè¿°è¿™å¼ å›¾ç‰‡"}
    
    å“åº”æ ¼å¼:
        {"caption": "å›¾ç‰‡æè¿°å†…å®¹"}
    """
    try:
        # æ„é€  base64 URL
        image_url = f"data:image/jpeg;base64,{request.image_base64}"
        
        # æ„å»ºæ¶ˆæ¯
        messages = [
            Message(
                role="user",
                content=[
                    ContentItem(type="image_url", image_url=ImageUrl(url=image_url)),
                    ContentItem(type="text", text=request.prompt)
                ]
            )
        ]
        
        # ç”Ÿæˆå›å¤
        output_text, prompt_tokens, completion_tokens = generate_response(
            messages,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
        )
        
        return CaptionResponse(
            caption=output_text,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens
        )
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


# ==================== åŸºç¡€ç«¯ç‚¹ ====================

@app.get("/")
async def root():
    return {
        "message": "Qwen3-VL VLM Service",
        "model": DEFAULT_MODEL_PATH,
        "endpoints": {
            "caption": "/caption (å…¼å®¹ imagemgr)",
            "chat": "/v1/chat/completions",
            "models": "/v1/models"
        }
    }


@app.get("/v1/models")
async def list_models():
    return {
        "object": "list",
        "data": [
            {
                "id": "qwen3-vl",
                "object": "model",
                "created": 1700000000,
                "owned_by": "qwen"
            }
        ]
    }


@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    """èŠå¤©è¡¥å…¨ API"""
    try:
        output_text, prompt_tokens, completion_tokens = generate_response(
            request.messages,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p,
            top_k=request.top_k,
        )
        
        response = ChatResponse(
            id=f"chatcmpl-{int(time.time())}",
            created=int(time.time()),
            model=request.model,
            choices=[
                Choice(
                    index=0,
                    message=Message(role="assistant", content=output_text),
                    finish_reason="stop"
                )
            ],
            usage=Usage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=prompt_tokens + completion_tokens
            )
        )
        
        return response
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


def main():
    parser = argparse.ArgumentParser(description="Qwen3-VL VLM æœåŠ¡")
    
    parser.add_argument(
        "--host", 
        type=str, 
        default="0.0.0.0",
        help="ç›‘å¬åœ°å€ (é»˜è®¤: 0.0.0.0)"
    )
    parser.add_argument(
        "--port", 
        type=int, 
        default=6050,
        help="ç›‘å¬ç«¯å£ (é»˜è®¤: 6050)"
    )
    parser.add_argument(
        "--model-path",
        type=str,
        default=DEFAULT_MODEL_PATH,
        help=f"æ¨¡å‹è·¯å¾„ (é»˜è®¤: {DEFAULT_MODEL_PATH})"
    )
    parser.add_argument(
        "--gpu",
        type=str,
        default="1",
        help="ä½¿ç”¨çš„ GPU è®¾å¤‡å· (é»˜è®¤: 1)"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½® GPUï¼ˆå¦‚æœç¯å¢ƒå˜é‡æœªè®¾ç½®åˆ™ä½¿ç”¨å‚æ•°ï¼‰
    if "CUDA_VISIBLE_DEVICES" not in os.environ:
        os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu
    os.environ["MODEL_PATH"] = args.model_path
    
    print("=" * 60)
    print("ğŸš€ Qwen3-VL VLM æœåŠ¡")
    print("=" * 60)
    print(f"ğŸ“¦ æ¨¡å‹: {args.model_path}")
    print(f"ğŸŒ åœ°å€: http://{args.host}:{args.port}")
    print(f"ğŸ® GPU: {args.gpu}")
    print("=" * 60)
    print()
    print("API ç«¯ç‚¹:")
    print(f"  POST http://{args.host}:{args.port}/caption  (imagemgr å…¼å®¹)")
    print(f"  POST http://{args.host}:{args.port}/v1/chat/completions")
    print(f"  GET  http://{args.host}:{args.port}/v1/models")
    print()
    print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    print("=" * 60)
    
    uvicorn.run(app, host=args.host, port=args.port)


if __name__ == "__main__":
    main()
