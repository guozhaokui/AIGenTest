from flask import Flask, request, jsonify, send_from_directory, Response, stream_with_context
from flask_cors import CORS
from openai import OpenAI
import os
import json
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½é¡¹ç›®æ ¹ç›®å½•çš„ .env æ–‡ä»¶
root_dir = Path(__file__).resolve().parent.parent.parent.parent
env_path = root_dir / '.env'
load_dotenv(dotenv_path=env_path)

print(f"Loading .env from: {env_path}")
print(f".env exists: {env_path.exists()}")

app = Flask(__name__)
CORS(app)

# NVIDIA API é…ç½®
NVIDIA_API_KEY = os.getenv('NVIDIA_API_KEY')
if not NVIDIA_API_KEY:
    raise ValueError("NVIDIA_API_KEY not found in environment variables")

client = OpenAI(
    base_url="https://integrate.api.nvidia.com/v1",
    api_key=NVIDIA_API_KEY
)

# å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼ˆæŒ‰ç”¨æˆ·åå¥½æ’åºï¼‰
AVAILABLE_MODELS = [
    # === ä¼˜å…ˆæ¨è ===
    {
        "id": "z-ai/glm4.7",
        "name": "ğŸ”¥ GLM-4.7",
        "description": "æ™ºè°± GLM-4.7ï¼Œä¸­æ–‡èƒ½åŠ›å¼º"
    },
    {
        "id": "minimaxai/minimax-m2.1",
        "name": "ğŸ”¥ MiniMax M2.1",
        "description": "MiniMax æœ€æ–°ç‰ˆæœ¬"
    },
    {
        "id": "moonshotai/kimi-k2-thinking",
        "name": "ğŸ”¥ Kimi K2 Thinking",
        "description": "æœˆä¹‹æš—é¢ Kimi K2 æ¨ç†æ¨¡å‹"
    },
    {
        "id": "deepseek-ai/deepseek-r1-0528",
        "name": "ğŸ”¥ DeepSeek R1 (0528)",
        "description": "DeepSeek R1 ç‰¹å®šç‰ˆæœ¬"
    },
    {
        "id": "deepseek-ai/deepseek-v3.2",
        "name": "ğŸ”¥ DeepSeek V3.2",
        "description": "DeepSeek æœ€æ–°ç‰ˆæœ¬ï¼Œå¼ºå¤§çš„é€šç”¨èƒ½åŠ›"
    },

    # === æ¨ç†æ¨¡å‹ï¼ˆReasoning Modelsï¼‰ ===
    {
        "id": "deepseek-ai/deepseek-r1",
        "name": "DeepSeek R1",
        "description": "æœ€æ–°æ¨ç†æ¨¡å‹ï¼Œç±»ä¼¼ OpenAI o1ï¼Œå¼ºå¤§çš„æ€ç»´é“¾èƒ½åŠ›"
    },
    {
        "id": "qwen/qwen3-next-80b-a3b-thinking",
        "name": "ğŸ”¥ Qwen3 Next 80B Thinking",
        "description": "Qwen3 æ¨ç†æ¨¡å‹ï¼Œæ”¯æŒæ·±åº¦æ€è€ƒ"
    },
    {
        "id": "deepseek-ai/deepseek-r1-distill-qwen-32b",
        "name": "DeepSeek R1 Distill Qwen 32B",
        "description": "R1 è’¸é¦ç‰ˆæœ¬ï¼ŒåŸºäº Qwen 32B"
    },
    {
        "id": "deepseek-ai/deepseek-r1-distill-qwen-14b",
        "name": "DeepSeek R1 Distill Qwen 14B",
        "description": "R1 è’¸é¦ç‰ˆæœ¬ï¼ŒåŸºäº Qwen 14B"
    },
    {
        "id": "deepseek-ai/deepseek-r1-distill-llama-8b",
        "name": "DeepSeek R1 Distill Llama 8B",
        "description": "R1 è’¸é¦ç‰ˆæœ¬ï¼ŒåŸºäº Llama 8B"
    },
    {
        "id": "qwen/qwq-32b",
        "name": "QwQ 32B",
        "description": "Qwen æ¨ç†æ¨¡å‹"
    },
    {
        "id": "microsoft/phi-4-mini-flash-reasoning",
        "name": "Phi-4 Mini Flash Reasoning",
        "description": "Microsoft æ¨ç†æ¨¡å‹"
    },

    # === æœ€æ–°ç‰ˆæœ¬å¤§æ¨¡å‹ ===
    {
        "id": "deepseek-ai/deepseek-v3.1",
        "name": "DeepSeek V3.1",
        "description": "DeepSeek ä¸Šä¸€ä»£æ——èˆ°æ¨¡å‹"
    },
    {
        "id": "meta/llama-4-maverick-17b-128e-instruct",
        "name": "ğŸ”¥ Llama 4 Maverick 17B (128 Experts)",
        "description": "Meta æœ€æ–° Llama 4ï¼Œ128ä¸“å®¶MoEæ¶æ„"
    },
    {
        "id": "meta/llama-4-scout-17b-16e-instruct",
        "name": "ğŸ”¥ Llama 4 Scout 17B (16 Experts)",
        "description": "Meta Llama 4ï¼Œ16ä¸“å®¶ç‰ˆæœ¬"
    },
    {
        "id": "meta/llama-3.3-70b-instruct",
        "name": "Llama 3.3 70B Instruct",
        "description": "Meta Llama 3.3ï¼Œæ”¹è¿›ç‰ˆ"
    },
    {
        "id": "mistralai/mistral-large-3-675b-instruct-2512",
        "name": "ğŸ”¥ Mistral Large 3 675B",
        "description": "Mistral æœ€æ–°æœ€å¤§æ¨¡å‹"
    },
    {
        "id": "mistralai/magistral-small-2506",
        "name": "Magistral Small",
        "description": "Mistral 2025å¹´6æœˆæ–°æ¨¡å‹"
    },
    {
        "id": "mistralai/devstral-2-123b-instruct-2512",
        "name": "Devstral 2 123B",
        "description": "Mistral å¼€å‘ä¸“ç”¨æ¨¡å‹"
    },
    {
        "id": "mistralai/ministral-14b-instruct-2512",
        "name": "Ministral 14B",
        "description": "Mistral å°å‹é«˜æ•ˆæ¨¡å‹"
    },
    {
        "id": "mistralai/mistral-small-3.1-24b-instruct-2503",
        "name": "Mistral Small 3.1 24B",
        "description": "Mistral Small æœ€æ–°ç‰ˆ"
    },
    {
        "id": "qwen/qwen3-coder-480b-a35b-instruct",
        "name": "ğŸ”¥ Qwen3 Coder 480B",
        "description": "Qwen3 ä»£ç æ¨¡å‹ï¼Œ480Bå‚æ•°"
    },
    {
        "id": "qwen/qwen3-235b-a22b",
        "name": "ğŸ”¥ Qwen3 235B",
        "description": "Qwen3 å¤§å‹æ¨¡å‹"
    },
    {
        "id": "qwen/qwen3-next-80b-a3b-instruct",
        "name": "Qwen3 Next 80B",
        "description": "Qwen3 Next ç³»åˆ—"
    },
    {
        "id": "google/gemma-3-27b-it",
        "name": "ğŸ”¥ Gemma 3 27B",
        "description": "Google æœ€æ–° Gemma 3"
    },
    {
        "id": "google/gemma-3-12b-it",
        "name": "Gemma 3 12B",
        "description": "Google Gemma 3 ä¸­ç­‰æ¨¡å‹"
    },
    {
        "id": "google/gemma-3-4b-it",
        "name": "Gemma 3 4B",
        "description": "Google Gemma 3 å°å‹æ¨¡å‹"
    },
    {
        "id": "microsoft/phi-4-multimodal-instruct",
        "name": "ğŸ”¥ Phi-4 Multimodal",
        "description": "Microsoft Phi-4 å¤šæ¨¡æ€æ¨¡å‹"
    },
    {
        "id": "microsoft/phi-4-mini-instruct",
        "name": "Phi-4 Mini",
        "description": "Microsoft Phi-4 å°å‹æ¨¡å‹"
    },
    {
        "id": "moonshotai/kimi-k2-instruct",
        "name": "Kimi K2 Instruct",
        "description": "æœˆä¹‹æš—é¢ Kimi K2"
    },

    # === NVIDIA æ¨¡å‹ ===
    {
        "id": "nvidia/llama-3.3-nemotron-super-49b-v1.5",
        "name": "ğŸ”¥ NVIDIA Nemotron Super 49B v1.5",
        "description": "NVIDIA æœ€æ–° Nemotronï¼ŒåŸºäº Llama 3.3"
    },
    {
        "id": "nvidia/llama-3.1-nemotron-ultra-253b-v1",
        "name": "NVIDIA Nemotron Ultra 253B",
        "description": "NVIDIA è¶…å¤§å‹æ¨¡å‹"
    },
    {
        "id": "nvidia/llama-3.1-nemotron-70b-instruct",
        "name": "NVIDIA Nemotron 70B",
        "description": "NVIDIA å¢å¼ºç‰ˆ Llama 70B"
    },
    {
        "id": "nvidia/llama-3.1-nemotron-51b-instruct",
        "name": "NVIDIA Nemotron 51B",
        "description": "NVIDIA ä¸­å‹é«˜æ•ˆæ¨¡å‹"
    },
    {
        "id": "nvidia/cosmos-reason2-8b",
        "name": "NVIDIA Cosmos Reason2 8B",
        "description": "NVIDIA æ¨ç†æ¨¡å‹"
    },

    # === ç»å…¸å¸¸ç”¨æ¨¡å‹ ===
    {
        "id": "meta/llama-3.1-405b-instruct",
        "name": "Llama 3.1 405B",
        "description": "Meta æœ€å¤§çš„ Llama 3.1 æ¨¡å‹"
    },
    {
        "id": "meta/llama-3.1-70b-instruct",
        "name": "Llama 3.1 70B",
        "description": "å¼ºå¤§çš„é€šç”¨æ¨¡å‹"
    },
    {
        "id": "meta/llama-3.1-8b-instruct",
        "name": "Llama 3.1 8B",
        "description": "å¿«é€Ÿé«˜æ•ˆçš„å°å‹æ¨¡å‹"
    },
    {
        "id": "mistralai/mistral-large-2-instruct",
        "name": "Mistral Large 2",
        "description": "Mistral å¤§å‹æ¨¡å‹"
    },
    {
        "id": "mistralai/mixtral-8x22b-instruct-v0.1",
        "name": "Mixtral 8x22B",
        "description": "æ··åˆä¸“å®¶æ¨¡å‹ï¼Œé«˜æ€§èƒ½"
    },
    {
        "id": "mistralai/mixtral-8x7b-instruct-v0.1",
        "name": "Mixtral 8x7B",
        "description": "ç»å…¸æ··åˆä¸“å®¶æ¨¡å‹"
    },
    {
        "id": "qwen/qwen2.5-coder-32b-instruct",
        "name": "Qwen2.5 Coder 32B",
        "description": "å¼ºå¤§çš„ä»£ç æ¨¡å‹"
    },
    {
        "id": "qwen/qwen2.5-7b-instruct",
        "name": "Qwen2.5 7B",
        "description": "Qwen 2.5 å°å‹æ¨¡å‹"
    },
    {
        "id": "google/gemma-2-27b-it",
        "name": "Gemma 2 27B",
        "description": "Google Gemma 2 å¤§å‹æ¨¡å‹"
    },
    {
        "id": "google/gemma-2-9b-it",
        "name": "Gemma 2 9B",
        "description": "é«˜æ•ˆçš„ä¸­å‹æ¨¡å‹"
    },
    {
        "id": "microsoft/phi-3.5-moe-instruct",
        "name": "Phi-3.5 MoE",
        "description": "Microsoft æ··åˆä¸“å®¶æ¨¡å‹"
    },
    {
        "id": "ibm/granite-3.3-8b-instruct",
        "name": "IBM Granite 3.3 8B",
        "description": "IBM æœ€æ–° Granite æ¨¡å‹"
    },
    {
        "id": "01-ai/yi-large",
        "name": "Yi Large",
        "description": "é›¶ä¸€ä¸‡ç‰©å¤§æ¨¡å‹"
    }
]

@app.route('/')
def index():
    return send_from_directory('.', 'index.html')

@app.route('/api/models', methods=['GET'])
def get_models():
    """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    return jsonify({"models": AVAILABLE_MODELS})

@app.route('/api/chat', methods=['POST'])
def chat():
    """å¤„ç†èŠå¤©è¯·æ±‚"""
    try:
        data = request.json
        model = data.get('model', 'meta/llama-3.1-8b-instruct')
        messages = data.get('messages', [])
        temperature = data.get('temperature', 0.7)
        max_tokens = data.get('max_tokens', 1024)

        if not messages:
            return jsonify({"error": "No messages provided"}), 400

        # è°ƒç”¨ NVIDIA API
        completion = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            stream=False
        )

        message = completion.choices[0].message
        response_content = message.content

        # è·å–æ€è€ƒå†…å®¹ï¼ˆä» model_extra ä¸­çš„ reasoning_content å­—æ®µï¼‰
        thinking_content = None
        if hasattr(message, 'model_extra') and message.model_extra:
            thinking_content = message.model_extra.get('reasoning_content')

        # å¦‚æœæ²¡æœ‰ reasoning_contentï¼Œå°è¯•ä» content ä¸­è§£æ <think> æ ‡ç­¾
        if not thinking_content and response_content:
            if '<think>' in response_content and '</think>' in response_content:
                import re
                think_pattern = r'<think>(.*?)</think>'
                matches = re.findall(think_pattern, response_content, re.DOTALL)
                if matches:
                    thinking_content = '\n\n'.join(matches).strip()
                    # ç§»é™¤æ€è€ƒæ ‡ç­¾ï¼Œåªä¿ç•™ç­”æ¡ˆ
                    response_content = re.sub(think_pattern, '', response_content, flags=re.DOTALL).strip()

        return jsonify({
            "success": True,
            "message": response_content,
            "thinking": thinking_content,
            "model": model
        })

    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@app.route('/api/chat/stream', methods=['POST'])
def chat_stream():
    """æµå¼èŠå¤©è¯·æ±‚"""
    try:
        data = request.json
        model = data.get('model', 'meta/llama-3.1-8b-instruct')
        messages = data.get('messages', [])
        temperature = data.get('temperature', 0.7)
        max_tokens = data.get('max_tokens', 4096)

        if not messages:
            return jsonify({"error": "No messages provided"}), 400

        def generate():
            try:
                # è°ƒç”¨ NVIDIA APIï¼ˆæµå¼ï¼‰
                stream = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    stream=True
                )

                for chunk in stream:
                    if chunk.choices and len(chunk.choices) > 0:
                        delta = chunk.choices[0].delta

                        # è·å–å†…å®¹
                        content = delta.content if hasattr(delta, 'content') else None

                        # è·å–æ€è€ƒå†…å®¹ï¼ˆä» model_extraï¼‰
                        reasoning = None
                        if hasattr(delta, 'model_extra') and delta.model_extra:
                            reasoning = delta.model_extra.get('reasoning_content')

                        # å‘é€æ•°æ®
                        if content or reasoning:
                            event_data = {
                                "content": content,
                                "reasoning": reasoning
                            }
                            yield f"data: {json.dumps(event_data, ensure_ascii=False)}\n\n"

                # å‘é€å®Œæˆä¿¡å·
                yield f"data: {json.dumps({'done': True})}\n\n"

            except Exception as e:
                yield f"data: {json.dumps({'error': str(e)})}\n\n"

        return Response(
            stream_with_context(generate()),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'X-Accel-Buffering': 'no'
            }
        )

    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

if __name__ == '__main__':
    print(f"Starting NVIDIA NIM Chat Server...")
    print(f"API Key configured: {NVIDIA_API_KEY[:10]}..." if NVIDIA_API_KEY else "No API Key")
    app.run(host='0.0.0.0', port=5000, debug=True)
