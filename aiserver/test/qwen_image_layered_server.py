"""
Qwen-Image-Layered Web æœåŠ¡
æä¾›å¯è§†åŒ–ç•Œé¢ï¼Œæ”¯æŒä¸Šä¼ å›¾ç‰‡è¿›è¡Œå›¾å±‚åˆ†è§£
"""

import os
import sys
import time
import torch
import numpy as np
import subprocess
from pathlib import Path
from PIL import Image
from typing import List, Optional, Tuple

import gradio as gr

# ç¦ç”¨ Gradio analyticsï¼ˆé¿å…ç½‘ç»œè¶…æ—¶è­¦å‘Šï¼‰
import os
os.environ["GRADIO_ANALYTICS_ENABLED"] = "False"

# ============== æ€§èƒ½ä¼˜åŒ–è®¾ç½® ==============
# 1. cuDNN ä¼˜åŒ–ï¼šå¯¹äºå›ºå®šè¾“å…¥å°ºå¯¸ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€å¿«çš„å·ç§¯ç®—æ³•
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False

# 2. å¯ç”¨ TF32ï¼ˆTensor Float 32ï¼‰- RTX 30ç³»åˆ—æ”¯æŒï¼ŒåŠ é€ŸçŸ©é˜µè¿ç®—
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# 3. è®¾ç½® float32 çŸ©é˜µä¹˜æ³•ç²¾åº¦ä¸º 'high'ï¼ˆä½¿ç”¨ TF32ï¼‰
torch.set_float32_matmul_precision('high')

print("âš¡ å·²å¯ç”¨æ€§èƒ½ä¼˜åŒ–: cuDNN benchmark + TF32")

# æ¨¡å‹è·¯å¾„
MODEL_PATH = "/data1/guo/AIGenTest/aiserver/models/Qwen/Qwen-Image-Layered"

# å…¨å±€å˜é‡
pipeline = None
model_loaded = False
use_int8 = False  # æ˜¯å¦ä½¿ç”¨ INT8 é‡åŒ–


def print_gpu_info():
    """æ‰“å° GPU ä¿¡æ¯"""
    print("\n" + "=" * 60)
    print("ğŸ–¥ï¸  GPU ä¿¡æ¯")
    print("=" * 60)
    
    # CUDA å¯ç”¨æ€§
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    
    if torch.cuda.is_available():
        print(f"å½“å‰ CUDA è®¾å¤‡: {torch.cuda.current_device()}")
        print(f"GPU æ•°é‡: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            total_mem = props.total_memory / 1024**3
            print(f"\n  GPU {i}: {props.name}")
            print(f"    - æ€»æ˜¾å­˜: {total_mem:.2f} GB")
            print(f"    - è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
            print(f"    - å¤šå¤„ç†å™¨æ•°: {props.multi_processor_count}")
    
    # ç¯å¢ƒå˜é‡
    cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES", "æœªè®¾ç½®")
    print(f"\nCUDA_VISIBLE_DEVICES: {cuda_visible}")
    print("=" * 60 + "\n")


def print_gpu_memory(prefix=""):
    """æ‰“å°å½“å‰ GPU æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    if not torch.cuda.is_available():
        print(f"{prefix}CUDA ä¸å¯ç”¨")
        return
    
    for i in range(torch.cuda.device_count()):
        allocated = torch.cuda.memory_allocated(i) / 1024**3
        reserved = torch.cuda.memory_reserved(i) / 1024**3
        total = torch.cuda.get_device_properties(i).total_memory / 1024**3
        print(f"{prefix}GPU {i}: å·²åˆ†é… {allocated:.2f}GB / å·²é¢„ç•™ {reserved:.2f}GB / æ€»å…± {total:.2f}GB")


def print_nvidia_smi():
    """è°ƒç”¨ nvidia-smi æ‰“å°ç®€æ´çš„ GPU çŠ¶æ€"""
    try:
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=index,name,memory.used,memory.total,power.draw,utilization.gpu",
             "--format=csv,noheader"],
            capture_output=True, text=True
        )
        if result.returncode == 0:
            print("ğŸ“Š nvidia-smi çŠ¶æ€:")
            print("   GPU | å‹å· | å·²ç”¨æ˜¾å­˜ | æ€»æ˜¾å­˜ | åŠŸç‡ | åˆ©ç”¨ç‡")
            for line in result.stdout.strip().split('\n'):
                print(f"   {line}")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¿è¡Œ nvidia-smi: {e}")


def load_model():
    """åŠ è½½æ¨¡å‹"""
    global pipeline, model_loaded, use_int8
    
    if model_loaded:
        return "æ¨¡å‹å·²åŠ è½½"
    
    # æ‰“å° GPU ä¿¡æ¯
    print_gpu_info()
    
    print("=" * 60)
    print("ğŸ”„ [1/3] åŠ è½½å‰æ˜¾å­˜çŠ¶æ€...")
    print_gpu_memory("   ")
    print_nvidia_smi()
    
    print("\nğŸ”„ [2/3] æ­£åœ¨åŠ è½½ Qwen-Image-Layered æ¨¡å‹...")
    print(f"   æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    t0 = time.time()
    
    from diffusers import QwenImageLayeredPipeline
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {MODEL_PATH}")
    
    # æ£€æŸ¥å¯ç”¨ GPU æ•°é‡
    num_gpus = torch.cuda.device_count()
    print(f"   å¯ç”¨ GPU æ•°é‡: {num_gpus}")
    print(f"   INT8 é‡åŒ–: {'å¯ç”¨' if use_int8 else 'ç¦ç”¨'}")
    
    # å‡†å¤‡é‡åŒ–é…ç½®
    quantization_config = None
    if use_int8:
        try:
            # diffusers ä½¿ç”¨è‡ªå·±çš„é‡åŒ–é…ç½®
            from diffusers.quantizers import PipelineQuantizationConfig
            # éœ€è¦æŒ‡å®š quant_mapping æ¥å‘Šè¯‰å“ªäº›ç»„ä»¶éœ€è¦é‡åŒ–
            quantization_config = PipelineQuantizationConfig(
                quant_backend="bitsandbytes_8bit",
                quant_kwargs={"load_in_8bit": True},
                # é‡åŒ– transformer å’Œ text_encoderï¼ˆæœ€å¤§çš„ä¸¤ä¸ªç»„ä»¶ï¼‰
                quant_mapping={
                    "transformer": {"load_in_8bit": True},
                    "text_encoder": {"load_in_8bit": True},
                },
            )
            print("   âœ… å·²é…ç½® INT8 é‡åŒ– (diffusers PipelineQuantizationConfig)")
            print("   ğŸ“‰ é¢„è®¡æ˜¾å­˜å ç”¨: ~27GB (åŸ 54GB çš„çº¦ 50%)")
        except (ImportError, TypeError, ValueError) as e:
            print(f"   âš ï¸ PipelineQuantizationConfig é…ç½®å¤±è´¥: {e}")
            # å°è¯•ä¸ä½¿ç”¨é‡åŒ–ï¼Œç›´æ¥å¤šå¡åŠ è½½
            print("   ğŸ’¡ å°†ä½¿ç”¨å¤šå¡å¹¶è¡Œä»£æ›¿é‡åŒ–")
            quantization_config = None
            use_int8 = False
    
    # æ¨¡å‹å¾ˆå¤§ï¼ˆ~54GBï¼‰ï¼Œéœ€è¦å¤šå¡åŠ è½½
    # transformer: 39GB, text_encoder: 16GB, vae: 243MB
    # INT8 é‡åŒ–åçº¦ 27GBï¼Œå•å¡ä»ç„¶ç´§å¼ ï¼Œå»ºè®® 2 å¡
    
    print(f"   ä½¿ç”¨ torch_dtype: bfloat16")
    
    # æ„å»ºåŠ è½½å‚æ•°
    # diffusers åªæ”¯æŒ "balanced" æˆ– "cuda"ï¼Œä¸æ”¯æŒ "auto"
    load_kwargs = {
        "torch_dtype": torch.bfloat16,
        "device_map": "balanced",
    }
    
    if quantization_config is not None:
        load_kwargs["quantization_config"] = quantization_config
    
    if use_int8 and quantization_config is not None and num_gpus >= 2:
        print(f"   âœ… INT8 é‡åŒ– + {num_gpus} å¼  GPUï¼Œä½¿ç”¨ device_map='balanced'")
        pipeline = QwenImageLayeredPipeline.from_pretrained(
            MODEL_PATH,
            **load_kwargs
        )
    elif use_int8 and quantization_config is not None and num_gpus == 1:
        print(f"   âš ï¸ INT8 é‡åŒ– + å•å¡æ¨¡å¼ï¼Œæ˜¾å­˜å¯èƒ½ä»ç„¶ç´§å¼ ")
        pipeline = QwenImageLayeredPipeline.from_pretrained(
            MODEL_PATH,
            **load_kwargs
        )
    elif num_gpus >= 3:
        print(f"   âœ… æ£€æµ‹åˆ° {num_gpus} å¼  GPUï¼Œåˆ†æ­¥åŠ è½½å„ç»„ä»¶")
        # æ¨¡å‹çº¦ 54GB (transformer 39GB, text_encoder 16GB, vae 0.24GB)
        # åˆ†åˆ«åŠ è½½å„ç»„ä»¶åˆ°ä¸åŒ GPU
        
        # æ–¹æ¡ˆï¼šä¸ä½¿ç”¨ device_mapï¼Œæ‰‹åŠ¨åˆ†é…
        # transformer (39GB) -> GPU 2,3,4 (è·¨å¡)
        # text_encoder (16GB) -> GPU 0
        # vae (0.24GB) -> GPU 1
        
        # åˆ†æ­¥åŠ è½½æ–¹å¼å¤ªå¤æ‚ï¼Œdiffusers pipeline æœ‰è‡ªå·±çš„ç»„ä»¶åŠ è½½é€»è¾‘
        # å›é€€åˆ°ä½¿ç”¨ pipeline çš„ç»Ÿä¸€åŠ è½½ï¼Œä½†å¼ºåˆ¶ä¸ä½¿ç”¨ CPU offload
        # é—®é¢˜çš„æ ¹æºæ˜¯ device_map="balanced" æŠŠ transformer æ”¾åˆ°äº† meta
        
        # å°è¯•ï¼šå…ˆå®Œæ•´åŠ è½½ pipelineï¼Œå†æ‰‹åŠ¨ç§»åŠ¨ transformer
        print("   [Step 1] å®Œæ•´åŠ è½½ pipeline...")
        pipeline = QwenImageLayeredPipeline.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.bfloat16,
            device_map="balanced",
        )
        
        # æ£€æŸ¥ transformer æ˜¯å¦åœ¨ meta è®¾å¤‡
        try:
            first_param = next(pipeline.transformer.parameters())
            if first_param.device.type == 'meta':
                print("   âš ï¸ transformer ä»åœ¨ meta è®¾å¤‡ï¼Œå°è¯•å¼ºåˆ¶åŠ è½½...")
                # å•ç‹¬åŠ è½½ transformer åˆ°å¤šå¡
                from diffusers import QwenImageTransformer2DModel
                transformer_path = os.path.join(MODEL_PATH, "transformer")
                
                # è·³è¿‡ GPU 0 (text_encoder ~16GB) å’Œ GPU 1 (vae ~0.24GB)
                # è®© transformer (~39GB) åˆ†å¸ƒåœ¨ GPU 2,3,4...
                transformer_max_memory = {}
                transformer_max_memory[0] = "0GiB"  # ä¸ç”¨ GPU 0
                transformer_max_memory[1] = "0GiB"  # ä¸ç”¨ GPU 1
                for i in range(2, num_gpus):
                    transformer_max_memory[i] = "22GiB"
                transformer_max_memory["cpu"] = "0GiB"  # ä¸å…è®¸ CPU offload
                
                print(f"   ğŸ“Š transformer max_memory: {transformer_max_memory}")
                
                new_transformer = QwenImageTransformer2DModel.from_pretrained(
                    transformer_path,
                    torch_dtype=torch.bfloat16,
                    device_map="balanced",
                    max_memory=transformer_max_memory,
                )
                pipeline.transformer = new_transformer
                print("   âœ… transformer é‡æ–°åŠ è½½å®Œæˆ")
        except StopIteration:
            print("   â„¹ï¸ transformer æ²¡æœ‰å‚æ•°")
        except Exception as e:
            print(f"   âš ï¸ æ£€æŸ¥/é‡è½½ transformer å¤±è´¥: {e}")
    elif num_gpus >= 2:
        print(f"   âš ï¸ ä»…æœ‰ {num_gpus} å¼  GPUï¼Œå°è¯•ä½¿ç”¨ device_map='balanced'")
        pipeline = QwenImageLayeredPipeline.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.bfloat16,
            device_map="balanced",
        )
    else:
        print(f"   âš ï¸ ä»…æœ‰ {num_gpus} å¼  GPU (24GB)ï¼Œæ¨¡å‹çº¦ 54GBï¼Œå¯èƒ½æ˜¾å­˜ä¸è¶³ï¼")
        print(f"   ğŸ’¡ å»ºè®®å¯ç”¨ INT8 é‡åŒ–: --int8")
        print(f"   å°è¯•ä½¿ç”¨ CPU offload...")
        pipeline = QwenImageLayeredPipeline.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.bfloat16,
            device_map="balanced",
            offload_folder="offload",
        )
    
    print(f"\n   âœ… Pipeline åŠ è½½å®Œæˆï¼Œè€—æ—¶: {time.time() - t0:.2f}ç§’")
    print(f"   ğŸ“¦ Pipeline ç±»å‹: {type(pipeline).__name__}")
    
    # æ£€æŸ¥ transformer æ˜¯å¦åœ¨ meta è®¾å¤‡ä¸Šï¼Œå¦‚æœæ˜¯åˆ™éœ€è¦æ‰‹åŠ¨ç§»åˆ° GPU
    if hasattr(pipeline, 'transformer') and pipeline.transformer is not None:
        try:
            first_param = next(pipeline.transformer.parameters())
            if first_param.device.type == 'meta':
                print("\n   âš ï¸ transformer åœ¨ meta è®¾å¤‡ä¸Šï¼Œå°è¯•æ‰‹åŠ¨åˆ†é…åˆ° GPU...")
                # ä½¿ç”¨ accelerate æ¥åˆ†é…åˆ°å¤šä¸ª GPU
                from accelerate import dispatch_model, infer_auto_device_map
                from accelerate.utils import get_balanced_memory
                
                # è®¡ç®—æ¯ä¸ª GPU å¯ç”¨çš„æ˜¾å­˜
                max_memory = get_balanced_memory(
                    pipeline.transformer,
                    max_memory=None,
                    no_split_module_classes=["QwenImageTransformerBlock"],
                    dtype=torch.bfloat16,
                )
                print(f"   ğŸ“Š è‡ªåŠ¨è®¡ç®—çš„ max_memory: {max_memory}")
                
                # æ¨æ–­è®¾å¤‡æ˜ å°„
                device_map = infer_auto_device_map(
                    pipeline.transformer,
                    max_memory=max_memory,
                    no_split_module_classes=["QwenImageTransformerBlock"],
                    dtype=torch.bfloat16,
                )
                print(f"   ğŸ“Š æ¨æ–­çš„ device_map å‰ 5 é¡¹: {dict(list(device_map.items())[:5])}")
                
                # åˆ†å‘æ¨¡å‹
                pipeline.transformer = dispatch_model(
                    pipeline.transformer,
                    device_map=device_map,
                )
                print("   âœ… transformer å·²åˆ†å‘åˆ° GPU")
        except StopIteration:
            print("   â„¹ï¸ transformer æ²¡æœ‰å‚æ•°")
        except Exception as e:
            print(f"   âš ï¸ å°è¯•ç§»åŠ¨ transformer å¤±è´¥: {e}")
    
    # æ‰“å° pipeline çš„ç»„ä»¶ä¿¡æ¯å’Œè®¾å¤‡åˆ†å¸ƒ
    print("\n   ğŸ“‹ Pipeline ç»„ä»¶åŠè®¾å¤‡åˆ†å¸ƒ:")
    for name, component in pipeline.components.items():
        if component is not None:
            component_type = type(component).__name__
            # æ£€æŸ¥æ˜¯å¦åœ¨ GPU ä¸Š
            device = "N/A"
            if hasattr(component, 'device'):
                device = str(component.device)
            elif hasattr(component, 'hf_device_map'):
                # å¤šå¡æ—¶å¯èƒ½æœ‰ device_map
                device = f"device_map: {component.hf_device_map}"
            elif hasattr(component, 'parameters'):
                try:
                    params = list(component.parameters())
                    if params:
                        devices = set(str(p.device) for p in params[:10])  # æ£€æŸ¥å‰10ä¸ªå‚æ•°
                        device = ", ".join(devices) if len(devices) <= 3 else f"{len(devices)} devices"
                except Exception:
                    device = "æ— æ³•è·å–"
            print(f"      - {name}: {component_type} (device: {device})")
    
    pipeline.set_progress_bar_config(disable=None)
    
    print(f"\nâœ… [2/3] æ¨¡å‹åŠ è½½å®Œæˆï¼Œæ€»è€—æ—¶: {time.time() - t0:.2f}ç§’")
    
    # åŠ è½½åæ˜¾å­˜çŠ¶æ€
    print("\nğŸ“Š åŠ è½½åæ˜¾å­˜çŠ¶æ€:")
    print_gpu_memory("   ")
    print_nvidia_smi()
    
    # ============== åŠ é€Ÿä¼˜åŒ–è¯´æ˜ ==============
    print("\nâš¡ å·²å¯ç”¨çš„åŠ é€Ÿä¼˜åŒ–:")
    print("   âœ… cuDNN benchmarkï¼ˆè‡ªåŠ¨é€‰æ‹©æœ€å¿«å·ç§¯ç®—æ³•ï¼‰")
    print("   âœ… TF32 çŸ©é˜µè¿ç®—åŠ é€Ÿï¼ˆRTX 30ç³»åˆ—ï¼‰")
    print("   âœ… Flash Attention 2.xï¼ˆæ¨¡å‹å†…ç½®ï¼‰")
    print("   âœ… bfloat16 æ··åˆç²¾åº¦")
    print("   âœ… å¤š GPU å¹¶è¡Œï¼ˆ5 å¼  RTX 3090ï¼‰")
    # æ³¨æ„ï¼šxformers ä¸æ­¤æ¨¡å‹ä¸å…¼å®¹ï¼ˆä¼šç ´ååŒè¾“å‡ºæ³¨æ„åŠ›æœºåˆ¶ï¼‰
    # æ³¨æ„ï¼štorch.compile å¯¹æ­¤æ¨¡å‹ä¸é€‚ç”¨ï¼ˆè®¡ç®—å›¾å¤ªæ·±ï¼‰
    
    # é¢„çƒ­
    print("\nğŸ”„ [3/3] é¢„çƒ­ä¸­ï¼ˆé¦–æ¬¡æ¨ç†ï¼Œç¼–è¯‘ CUDA kernelï¼‰...")
    t0 = time.time()
    dummy_image = Image.new("RGBA", (256, 256), (128, 128, 128, 255))
    with torch.inference_mode():
        _ = pipeline(
            image=dummy_image,
            generator=torch.Generator(device='cuda').manual_seed(0),
            num_inference_steps=2,
            layers=2,
            resolution=640,
        )
    print(f"âœ… [3/3] é¢„çƒ­å®Œæˆï¼Œè€—æ—¶: {time.time() - t0:.2f}ç§’")
    
    # é¢„çƒ­åæ˜¾å­˜çŠ¶æ€
    print("\nğŸ“Š é¢„çƒ­åæ˜¾å­˜çŠ¶æ€:")
    print_gpu_memory("   ")
    print_nvidia_smi()
    
    model_loaded = True
    print("\n" + "=" * 60)
    print("ğŸ‰ æ¨¡å‹åŠ è½½å®Œæˆï¼æœåŠ¡å³å°†å¯åŠ¨...")
    print("=" * 60)
    return "æ¨¡å‹åŠ è½½å®Œæˆ"


def process_image(
    image: np.ndarray,
    num_layers: int,
    num_inference_steps: int,
    true_cfg_scale: float,
    resolution: int,
    seed: int,
    cfg_normalize: bool,
    use_en_prompt: bool,
    negative_prompt: str,
    progress=gr.Progress()
) -> Tuple[List[Image.Image], Image.Image, str]:
    """å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡ï¼Œåˆ†è§£ä¸ºå¤šä¸ªå›¾å±‚"""
    import datetime
    print(f"\nâ° [{datetime.datetime.now().strftime('%H:%M:%S.%f')[:-3]}] process_image() å‡½æ•°è¢«è°ƒç”¨")
    
    global pipeline, model_loaded
    
    if not model_loaded:
        progress(0, desc="åŠ è½½æ¨¡å‹ä¸­...")
        load_model()
    
    if image is None:
        return None, None, "âŒ è¯·å…ˆä¸Šä¼ å›¾ç‰‡"
    
    start_time = time.time()
    print("\n" + "-" * 40)
    print(f"ğŸ“· æ”¶åˆ°æ–°å›¾ç‰‡ï¼Œå°ºå¯¸: {image.shape}")
    
    try:
        # è½¬æ¢ä¸º PIL Image (RGBA)
        if len(image.shape) == 2:
            # ç°åº¦å›¾è½¬RGB
            image = np.stack([image] * 3, axis=-1)
        
        if image.shape[2] == 3:
            # RGB è½¬ RGBA
            pil_image = Image.fromarray(image).convert("RGBA")
        else:
            pil_image = Image.fromarray(image)
        
        print(f"ğŸ“ å›¾ç‰‡è½¬æ¢ä¸º RGBAï¼Œå°ºå¯¸: {pil_image.size}")
        
        # è®¾ç½®å‚æ•°
        inputs = {
            "image": pil_image,
            "generator": torch.Generator(device='cuda').manual_seed(seed),
            "true_cfg_scale": true_cfg_scale,
            "negative_prompt": negative_prompt if negative_prompt.strip() else " ",
            "num_inference_steps": num_inference_steps,
            "num_images_per_prompt": 1,
            "layers": num_layers,
            "resolution": resolution,
            "cfg_normalize": cfg_normalize,
            "use_en_prompt": use_en_prompt,
        }
        
        print(f"âš™ï¸ å‚æ•°: layers={num_layers}, steps={num_inference_steps}, cfg={true_cfg_scale}, resolution={resolution}")
        
        # æ¨ç†å‰æ˜¾å­˜çŠ¶æ€
        print("\nğŸ“Š æ¨ç†å‰æ˜¾å­˜çŠ¶æ€:")
        print_gpu_memory("   ")
        
        # æ¨ç†
        progress(0.2, desc="æ­£åœ¨åˆ†è§£å›¾å±‚...")
        print("ğŸ”„ æ­£åœ¨æ¨ç†...")
        t0 = time.time()
        
        with torch.inference_mode():
            output = pipeline(**inputs)
            output_images = output.images[0]  # List of PIL Images (RGBA)
        
        # åŒæ­¥ GPU ç¡®ä¿è®¡æ—¶å‡†ç¡®
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        
        inference_time = time.time() - t0
        print(f"âœ… æ¨ç†å®Œæˆï¼Œè€—æ—¶: {inference_time:.2f}ç§’ï¼Œç”Ÿæˆ {len(output_images)} ä¸ªå›¾å±‚")
        
        # æ¨ç†åæ˜¾å­˜çŠ¶æ€
        print("\nğŸ“Š æ¨ç†åæ˜¾å­˜çŠ¶æ€:")
        print_gpu_memory("   ")
        print_nvidia_smi()
        
        # åˆæˆé¢„è§ˆå›¾ï¼ˆå°†æ‰€æœ‰å›¾å±‚å åŠ ï¼‰
        progress(0.9, desc="ç”Ÿæˆé¢„è§ˆ...")
        composite = None
        for layer in output_images:
            if composite is None:
                composite = layer.copy()
            else:
                composite = Image.alpha_composite(composite, layer)
        
        total_time = time.time() - start_time
        print(f"ğŸ‰ å¤„ç†å®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print("-" * 40)
        
        # ç”Ÿæˆç»“æœæ‘˜è¦
        summary = f"""## ğŸ“Š åˆ†è§£ç»“æœ

**å›¾å±‚æ•°é‡**: {len(output_images)}
**æ¨ç†è€—æ—¶**: {inference_time:.2f} ç§’
**æ€»å¤„ç†è€—æ—¶**: {total_time:.2f} ç§’

### å‚æ•°é…ç½®
- **æ¨ç†æ­¥æ•°**: {num_inference_steps}
- **CFG Scale**: {true_cfg_scale}
- **åˆ†è¾¨ç‡**: {resolution}
- **éšæœºç§å­**: {seed}
- **CFG å½’ä¸€åŒ–**: {'æ˜¯' if cfg_normalize else 'å¦'}
- **ä½¿ç”¨è‹±æ–‡æç¤º**: {'æ˜¯' if use_en_prompt else 'å¦'}

### å›¾å±‚ä¿¡æ¯
"""
        for i, layer in enumerate(output_images):
            # è®¡ç®—å›¾å±‚çš„éé€æ˜åƒç´ æ¯”ä¾‹
            alpha = np.array(layer.split()[-1])
            non_transparent = np.sum(alpha > 0) / alpha.size * 100
            summary += f"- **Layer {i}**: {layer.size[0]}x{layer.size[1]}, éé€æ˜åŒºåŸŸ: {non_transparent:.1f}%\n"
        
        return output_images, composite, summary
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ å¤„ç†å‡ºé”™: {str(e)}\n\n```\n{traceback.format_exc()}\n```"
        print(error_msg)
        return None, None, error_msg


def save_layers(layers: List[Image.Image]) -> Optional[str]:
    """ä¿å­˜æ‰€æœ‰å›¾å±‚ä¸º ZIP æ–‡ä»¶"""
    if layers is None or len(layers) == 0:
        return None
    
    import zipfile
    import tempfile
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = tempfile.mkdtemp()
    zip_path = os.path.join(temp_dir, "layers.zip")
    
    with zipfile.ZipFile(zip_path, 'w') as zipf:
        for i, layer in enumerate(layers):
            layer_path = os.path.join(temp_dir, f"layer_{i}.png")
            layer.save(layer_path, "PNG")
            zipf.write(layer_path, f"layer_{i}.png")
    
    return zip_path


def create_interface():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    with gr.Blocks(
        title="Qwen-Image-Layered - å›¾åƒåˆ†å±‚åˆ†è§£",
        theme=gr.themes.Soft(
            primary_hue="indigo",
            secondary_hue="slate",
            neutral_hue="slate",
        ),
        css="""
        .main-title {
            text-align: center;
            margin-bottom: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .layer-gallery img {
            border: 2px solid #ddd;
            border-radius: 8px;
        }
        .result-box {
            min-height: 300px;
        }
        footer {
            display: none !important;
        }
        """
    ) as demo:
        
        # ç”¨äºå­˜å‚¨å›¾å±‚ç»“æœ
        layers_state = gr.State([])
        
        gr.HTML("""
        <div class="main-title">
            <h1>ğŸ¨ Qwen-Image-Layered</h1>
            <p style="color: #666;">æ™ºèƒ½å›¾åƒåˆ†å±‚åˆ†è§£ - å°†å›¾åƒåˆ†è§£ä¸ºå¯ç‹¬ç«‹ç¼–è¾‘çš„ RGBA å›¾å±‚</p>
        </div>
        """)
        
        with gr.Row():
            # å·¦ä¾§ï¼šè¾“å…¥åŒºåŸŸ
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“¤ ä¸Šä¼ å›¾ç‰‡")
                input_image = gr.Image(
                    label="è¾“å…¥å›¾ç‰‡",
                    type="numpy",
                    height=350
                )
                
                with gr.Accordion("âš™ï¸ å‚æ•°è®¾ç½®", open=True):
                    num_layers = gr.Slider(
                        minimum=2,
                        maximum=10,
                        value=4,
                        step=1,
                        label="å›¾å±‚æ•°é‡",
                        info="åˆ†è§£ä¸ºå¤šå°‘ä¸ªå›¾å±‚ï¼ˆ2-10ï¼‰"
                    )
                    
                    num_inference_steps = gr.Slider(
                        minimum=10,
                        maximum=100,
                        value=50,
                        step=5,
                        label="æ¨ç†æ­¥æ•°",
                        info="æ­¥æ•°è¶Šå¤šè´¨é‡è¶Šé«˜ï¼Œä½†é€Ÿåº¦è¶Šæ…¢"
                    )
                    
                    true_cfg_scale = gr.Slider(
                        minimum=1.0,
                        maximum=10.0,
                        value=4.0,
                        step=0.5,
                        label="CFG Scale",
                        info="æ§åˆ¶ç”Ÿæˆçš„å¼•å¯¼å¼ºåº¦"
                    )
                    
                    resolution = gr.Radio(
                        choices=[640, 1024],
                        value=640,
                        label="åˆ†è¾¨ç‡",
                        info="640 æ¨èç”¨äºæµ‹è¯•ï¼Œ1024 ç”¨äºé«˜è´¨é‡è¾“å‡º"
                    )
                    
                    seed = gr.Number(
                        value=777,
                        label="éšæœºç§å­",
                        info="å›ºå®šç§å­å¯å¤ç°ç»“æœ"
                    )
                    
                    with gr.Row():
                        cfg_normalize = gr.Checkbox(
                            value=True,
                            label="CFG å½’ä¸€åŒ–"
                        )
                        use_en_prompt = gr.Checkbox(
                            value=True,
                            label="ä½¿ç”¨è‹±æ–‡æç¤º"
                        )
                    
                    negative_prompt = gr.Textbox(
                        value=" ",
                        label="è´Ÿé¢æç¤ºè¯ï¼ˆå¯é€‰ï¼‰",
                        placeholder="ä¸å¸Œæœ›å‡ºç°çš„å†…å®¹..."
                    )
                
                with gr.Row():
                    submit_btn = gr.Button("ğŸš€ å¼€å§‹åˆ†è§£", variant="primary", size="lg")
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤", size="lg")
            
            # å³ä¾§ï¼šè¾“å‡ºåŒºåŸŸ
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ–¼ï¸ åˆæˆé¢„è§ˆ")
                output_composite = gr.Image(
                    label="å›¾å±‚åˆæˆé¢„è§ˆ",
                    height=350
                )
                
                gr.Markdown("### ğŸ“š åˆ†å±‚ç»“æœ")
                output_gallery = gr.Gallery(
                    label="å„å›¾å±‚é¢„è§ˆ",
                    columns=4,
                    rows=2,
                    height=200,
                    object_fit="contain",
                    elem_classes=["layer-gallery"]
                )
                
                download_btn = gr.Button("ğŸ“¥ ä¸‹è½½æ‰€æœ‰å›¾å±‚ (ZIP)", size="lg")
                download_file = gr.File(label="ä¸‹è½½æ–‡ä»¶", visible=False)
        
        with gr.Row():
            with gr.Column():
                gr.Markdown("### ğŸ“‹ å¤„ç†ç»“æœ")
                output_summary = gr.Markdown(
                    value="ç­‰å¾…å¤„ç†...",
                    elem_classes=["result-box"]
                )
        
        gr.Markdown("""
        ---
        ### ğŸ’¡ ä½¿ç”¨è¯´æ˜
        
        1. **ä¸Šä¼ å›¾ç‰‡**ï¼šæ”¯æŒ PNGã€JPG ç­‰å¸¸è§æ ¼å¼
        2. **è°ƒæ•´å‚æ•°**ï¼šæ ¹æ®éœ€è¦è°ƒæ•´å›¾å±‚æ•°é‡å’Œè´¨é‡å‚æ•°
        3. **å¼€å§‹åˆ†è§£**ï¼šç‚¹å‡»æŒ‰é’®ï¼Œç­‰å¾…æ¨¡å‹å¤„ç†
        4. **æŸ¥çœ‹ç»“æœ**ï¼šé¢„è§ˆå„ä¸ªå›¾å±‚ï¼Œä¸‹è½½ ZIP åŒ…è¿›è¡Œåç»­ç¼–è¾‘
        
        **åº”ç”¨åœºæ™¯**ï¼š
        - ğŸ¨ å›¾åƒç¼–è¾‘ï¼šç‹¬ç«‹è°ƒæ•´å‰æ™¯/èƒŒæ™¯
        - ğŸ”„ å¯¹è±¡æ›¿æ¢ï¼šæ›¿æ¢ç‰¹å®šå›¾å±‚å†…å®¹
        - ğŸ—‘ï¸ å¯¹è±¡åˆ é™¤ï¼šç§»é™¤ä¸éœ€è¦çš„å›¾å±‚
        - ğŸ“ é‡æ–°å¸ƒå±€ï¼šè‡ªç”±ç§»åŠ¨å„å›¾å±‚ä½ç½®
        """)
        
        # äº‹ä»¶ç»‘å®š
        def process_and_store(image, num_layers, num_inference_steps, true_cfg_scale, 
                             resolution, seed, cfg_normalize, use_en_prompt, negative_prompt):
            layers, composite, summary = process_image(
                image, num_layers, num_inference_steps, true_cfg_scale,
                resolution, int(seed), cfg_normalize, use_en_prompt, negative_prompt
            )
            return layers, composite, layers, summary
        
        submit_btn.click(
            fn=process_and_store,
            inputs=[
                input_image, num_layers, num_inference_steps, true_cfg_scale,
                resolution, seed, cfg_normalize, use_en_prompt, negative_prompt
            ],
            outputs=[output_gallery, output_composite, layers_state, output_summary]
        )
        
        clear_btn.click(
            fn=lambda: (None, None, None, [], "ç­‰å¾…å¤„ç†..."),
            inputs=[],
            outputs=[input_image, output_composite, output_gallery, layers_state, output_summary]
        )
        
        download_btn.click(
            fn=save_layers,
            inputs=[layers_state],
            outputs=[download_file]
        )
    
    return demo


def main():
    """å¯åŠ¨æœåŠ¡"""
    global use_int8
    import argparse
    
    parser = argparse.ArgumentParser(description="Qwen-Image-Layered WebæœåŠ¡")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="æœåŠ¡åœ°å€")
    parser.add_argument("--port", type=int, default=7861, help="æœåŠ¡ç«¯å£")
    parser.add_argument("--share", action="store_true", help="åˆ›å»ºå…¬å…±é“¾æ¥")
    parser.add_argument("--preload", action="store_true", help="å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹")
    parser.add_argument("--int8", action="store_true", help="å¯ç”¨ INT8 é‡åŒ–ï¼ˆå‡å°‘çº¦ 50% æ˜¾å­˜ï¼‰")
    args = parser.parse_args()
    
    # è®¾ç½®é‡åŒ–é€‰é¡¹
    use_int8 = args.int8
    
    print("=" * 60)
    print("ğŸ¨ Qwen-Image-Layered Web æœåŠ¡")
    print("=" * 60)
    print(f"ğŸ“‚ æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    print(f"ğŸ”¢ INT8 é‡åŒ–: {'âœ… å¯ç”¨' if use_int8 else 'âŒ ç¦ç”¨'}")
    
    # å¯åŠ¨æ—¶æ‰“å° GPU ä¿¡æ¯
    print_gpu_info()
    print_nvidia_smi()
    
    # é¢„åŠ è½½æ¨¡å‹
    if args.preload:
        load_model()
    
    # åˆ›å»ºç•Œé¢
    demo = create_interface()
    
    print(f"\nğŸŒ æœåŠ¡åœ°å€: http://{args.host}:{args.port}")
    print("=" * 60)
    
    # å¯åŠ¨æœåŠ¡
    demo.launch(
        server_name=args.host,
        server_port=args.port,
        share=args.share,
        show_error=True
    )


if __name__ == "__main__":
    main()

